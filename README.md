# FutureMakers2022

NOTE: Some notebooks contain errors as runtime was forcefully disconnected when it was realized running the models with a CPU instead of a GPU (as I did not have google colab pro) would take too long.

## Day 1 Reflections

I learned about the different ways one can create arrays in Python as I haven't used Python too much in complex problems due to my school not having class on Python. It was also good to refresh on the basics of Python as I've always ended up having to force myself to use the benefits that Python give in the ease and cleanliness that Python codes provide as I'm used to coding in more explicit languages.

## Day 2 Reflections

Today, I learned about the different types of machine learning: supervised, unsupervised, semi-supervised, and reinforcement learning. In particular, learning about semi-supervised was easy for me because an example of computer vision's object identification came into my head; if a computer was trying to sort pictures of cats and dogs, you might label some pictures as dogs and some as cats to give a baseline for the computer, then let the algorithm look at a bunch of unlabeled pictures to create a model.

## Day 3 Reflections

Today was really interesting because I could really see machine learning in action and how you can utilize machine learning to categorize data points. I knew about linear regression, but perceptron and kmeans algorithms was new and interesting to me. The methods they utilized to categorize data points were fascinating. We also dived into actual machine learning today, and how an architecture is formatted.

## Day 4 Reflections

Today we learned about TensorFlow and the different functionalities within. It was really fascinating to see how someone managed to create an entire new class of variables to make machine learning easier, and it made me realize just how much there was to learn about machine learning. It was also really interesting to see machine learning in action; I never knew computer vision's logic was so simple yet elegent. 

## Day 5 Reflections

Today was a bit difficult due to the various jargons being thrown around, but 3blue1brown really hellped me on this. It's interesting to see how neural networks can seem so simple once you understand that it's just a series of layers controlled by activation layers and weights. It was also interesting to see the concept of Stochastic Gradient Descent as I've learned of it in math class but never thought I'd see it here.

## Day 6 Reflections

Once again, today was a day filled with difficult topics. Essentially, CNNs compress the images to try and find "features" which are essentially patterns, and by having multiple layers, it is able to find and identify more and more difficult patterns. 

## Day 7 Reflections

I've always understood algorithmic bias as it was taught in my class extensively, but seeing it truely in action surprised just how subtle it could be. It made me realize just how careful one has to be in defending against bias. I also understood more of why it might occur. After all, people only have so much time to waste, so we start to look for patterns that don't really exist. And if we utilize patterns we've made up, we create bias for our algorithm.

## Day 8 Reflections

While I understood the concept of overfitting and underfitting, it was a bit hard to visualize exactly what they were until I saw the articles linked at the end of the notebook that gave a great visualization. It was interesting to see the methods to combat these problems such as dropout and padding, as I would've never thought of such things. 

## Day 9 Reflections

I can see how this might be, by far, the hardest day if you haven't progressed far enough in math courses, as some of our fellow Futuremaker members were. However, with the appropriate knowledge, it was easy to see that the loss functions were simply processing values through common functions to scale and limit values. I was a bit confused on one-hot encoding, however. 

## Day 10 Reflections

The activations functions were comparatively easy to understand. The main reason for these functions were to defend against exploding and vanishing gradients as we try to limit the range of the functions. The only difficult part was understand exactly what the activation functions were.

## Day 11 Reflections

Because of my school's insistence on ethical programming, much of this was, once again, familiar to me. Feedback loop did interest me as it kind of felt like another similar, yet unrelated topic, confirmation bias. Once again, I've confirmed that it's critical to find many different methods to prevent any problems as many times, the reason ethics is a problem in programming isn't intentional, but simply that one couldn't recognize the problem.

## Day 12 Reflections
This was the first time in the program where it felt like we had the chance to really practice making a neural network by ourselves. While I did think it was a bit late to do this, it was indeed helpful. I'd like to see how unsupervised learning works, as this was an example of supervised model, and I knew how it worked.

## Day 13 Reflections

While I understood overfitting and underfitting's concept, I didn't understand how to identify it in a real-life setting. Seeing the graphs and the explanations really helped. Knowing bits and varience also really helped me categorize what kind of issues can lead to overfitting and underfitting.

## Day 14 Reflections
Today was a very easy day as encoders and decoders were very simple concepts to grasp. However, thinking how it may become a generator was interesting. Seeing a layer reshaped was very strange as it showed me that I shouldn't think of machine learning in human thought processes even if it may try to accomplish the same things.

## Day 15 Reflections

This day made me wish we tried messing around with the data a bit more to try and format them in the way we want. Additionally, affective computing was an interesting topic for me as I'm deeply interested in the world of sentiment analysis, specifically in the NLP side. It was very intersting to see sound data changed to look 2d.

## Day 16 Reflections

Today was a fascinating day for me as I know most about NLP. Most topics in the notebook were known to me, though seeing it through the lens of machine learning as I've learned it in this program made me rethink my previous thoughts regarding NLP models.

## Day 17 Reflections

The last day had a lot of higher level knowledge, in my opinion. It took us far ahead of what we were taught and showed us some more advanced computer vision such as style transfer and GANs. This made me feel a bit overwhelmed, but I was interested the whole way through as we've dealt with computer vision the most in this program. However, there were some problems such as having latent_dim undefined, which me and my group later realized should've been defined in an answer to a question which could've actully been answered multiple ways. This made me think the notebook today was a bit underdeveloped for teaching. 
